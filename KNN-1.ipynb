{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a3465b7-9950-478f-bd28-457e974b1360",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Q1. What is the KNN algorithm?\n",
    "\n",
    "#K-Nearest Neighbors (KNN) Algorithm\n",
    "#K-Nearest Neighbors (KNN) is a simple, non-parametric, and lazy learning algorithm that is used for both classification and regression tasks. It's based on the principle of similarity, where the class or value of a data point is determined by the majority vote or average of its closest neighbors.\n",
    "\n",
    "#Key Concepts\n",
    "#K:\n",
    "\n",
    "#Represents the number of neighbors to consider for making the prediction. It's a hyperparameter that you need to set before running the algorithm.\n",
    "#Distance Metric:\n",
    "\n",
    "#The distance between data points is calculated to identify the closest neighbors. Common metrics include:\n",
    "#Euclidean distance (for continuous features)\n",
    "#Manhattan distance\n",
    "#Minkowski distance\n",
    "#Classification:\n",
    "\n",
    "#In classification, the algorithm assigns a class label based on the majority class among the K nearest neighbors.\n",
    "#Majority voting is often used. For example, if 3 out of 5 neighbors belong to class A and 2 belong to class B, the algorithm will classify the new point as class A.\n",
    "#Regression:\n",
    "\n",
    "#In regression, KNN predicts the value by averaging the values of the K nearest neighbors.\n",
    "#Lazy Learning:\n",
    "\n",
    "#KNN is a lazy learning algorithm, meaning it does not learn a model from the training data explicitly. Instead, it memorizes the training dataset and performs computations only when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59f1162d-ddf6-4b60-a8bc-9bce5ab84961",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "#Choosing the value of K in KNN (K-Nearest Neighbors) involves balancing the trade-off between bias and variance. Here are some methods to help you choose the optimal K:\n",
    "\n",
    "#1. Cross-Validation: Split your data into training and testing sets. Try different K values and evaluate the model's performance using metrics like accuracy or mean squared error. Choose the K that results in the best performance.\n",
    "\n",
    "#2. K-Value Range: Start with a small K (e.g., 1, 3, 5) and incrementally increase it. Monitor the performance and stop when it starts to degrade.\n",
    "\n",
    "#3. Elbow Method: Plot the error rate or accuracy against different K values. The optimal K is where the curve starts to flatten (the \"elbow point\").\n",
    "\n",
    "#4. Rule of Thumb: Use K = √n, where n is the number of data points.\n",
    "\n",
    "#5. Domain Knowledge: Choose K based on the specific problem or domain expertise.\n",
    "\n",
    "#6. Hyperparameter Tuning: Use techniques like Grid Search, Random Search, or Bayesian Optimization to find the optimal K.\n",
    "\n",
    "#Remember, the optimal K value depends on the dataset and problem. Experimenting with different values and evaluating the model's performance is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "749c0a63-e238-4bd3-ae58-ae6aa8df69f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "##If you're asking about the difference between KNN classifier and KNN regressor, here's a concise answer:\n",
    "\n",
    "#KNN Classifier:\n",
    "\n",
    "#- Predicts a categorical label (class) for a new data point\n",
    "#- Uses the majority vote of the K nearest neighbors to determine the class label\n",
    "#- Suitable for classification problems (e.g., spam vs. non-spam emails, cancer diagnosis)\n",
    "\n",
    "#KNN Regressor:\n",
    "\n",
    "#- Predicts a continuous value (regression) for a new data point\n",
    "#- Uses the average or weighted average of the K nearest neighbors to predict the value\n",
    "#- Suitable for regression problems (e.g., predicting house prices, stock prices)\n",
    "\n",
    "#In summary:\n",
    "\n",
    "#- Classifier: Predicts a class label (categorical)\n",
    "#- Regressor: Predicts a continuous value (regression)\n",
    "\n",
    "#Both use the same KNN algorithm, but the key difference lies in the output and problem type they address.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e26b4bd-8df3-4163-a67f-eb4d6b6e4160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. How do you measure the performance of KNN?\n",
    "\n",
    "#Here are some common metrics to measure the performance of KNN:\n",
    "\n",
    "#H#ere are some common metrics to measure the performance of KNN:\n",
    "\n",
    "#Classification:\n",
    "\n",
    "#1. Accuracy: Proportion of correctly classified instances.\n",
    "2. Precision: Proportion of true positives (correctly classified instances) among all positive predictions.\n",
    "3. Recall: Proportion of true positives among all actual positive instances.\n",
    "4. F1-score: Harmonic mean of precision and recall.\n",
    "5. Confusion Matrix: Table showing true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "Regression:\n",
    "\n",
    "1. Mean Squared Error (MSE): Average squared difference between predicted and actual values.\n",
    "2. Mean Absolute Error (MAE): Average absolute difference between predicted and actual values.\n",
    "3. Root Mean Squared Error (RMSE): Square root of MSE.\n",
    "4. R-squared (R²): Measures the proportion of variance explained by the model.\n",
    "5. Mean Absolute Percentage Error (MAPE): Average absolute percentage difference between predicted and actual values.\n",
    "\n",
    "Additional metrics:\n",
    "\n",
    "1. Cross-validation score: Evaluates model performance on unseen data.\n",
    "2. ROC-AUC curve: Plots true positive rate against false positive rate for classification.\n",
    "3. Training time and prediction time: Measures computational efficiency.\n",
    "\n",
    "Choose metrics relevant to your problem and dataset to evaluate KNN's performance effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
